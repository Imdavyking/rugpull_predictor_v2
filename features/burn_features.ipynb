{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "burns_df = pd.read_csv('burns.csv', low_memory=False)\n",
    "rugpull_labels_df = pd.read_csv('rugpulls_with_token_info.csv', low_memory=False)\n",
    "\n",
    "# Ensure that 'id' is the correct identifier for pools\n",
    "print(burns_df.head())\n",
    "\n",
    "# Convert 'date' to a datetime object\n",
    "burns_df['timestamp'] = pd.to_datetime(burns_df['timestamp'], unit='s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the feature rows\n",
    "feature_rows = []\n",
    "# Initialize an empty DataFrame for burn features\n",
    "burn_features = pd.DataFrame()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "count = 0\n",
    "# Initialize a list to store the feature rows\n",
    "feature_rows = []\n",
    "# Initialize a DataFrame to store the features\n",
    "pool_features = pd.DataFrame()\n",
    "\n",
    "for index, pool_id in enumerate(rugpull_labels_df['pool_id']):\n",
    "    if index % 200 == 0:\n",
    "        print(f\"Processing pool {index}: Pool ID {pool_id}\")\n",
    "        print(f\"Number of pools skipped: {count}\")\n",
    "        # if index > 0: break\n",
    "\n",
    "    rugpull_date = rugpull_labels_df[rugpull_labels_df['pool_id'] == pool_id]['date'].iloc[0] # Get the rugpull date for this pool\n",
    "\n",
    "    token0_symbol = rugpull_labels_df[rugpull_labels_df['pool_id'] == pool_id]['token0.symbol'].iloc[0]\n",
    "    token1_symbol = rugpull_labels_df[rugpull_labels_df['pool_id'] == pool_id]['token1.symbol'].iloc[0]\n",
    "\n",
    "    # print(rugpull_date)\n",
    "    burn_data = burns_df[burns_df['pool.id'] == pool_id]\n",
    "\n",
    "    # if (len(burn_data) < 3):\n",
    "    #     # print(f\"Skipping pool {pool_id} because there is no data\")\n",
    "    #     count += 1\n",
    "    #     continue\n",
    "\n",
    "    # If rugpull_date is NaT, use all available data; else filter data before rugpull\n",
    "    pre_rugpull_data = burn_data if pd.isna(rugpull_date) else burn_data[burn_data['timestamp'] < rugpull_date]\n",
    "    \n",
    "    if pre_rugpull_data.empty:\n",
    "        print(f\"Skipping pool {pool_id} because there is no data before the rugpull date\")\n",
    "        continue\n",
    "\n",
    "    # # Calculate the distribution of burned tokens for the top 10 addresses\n",
    "    # grouped_owners = pre_rugpull_data.groupby('origin')['amount'].sum()\n",
    "    # print(grouped_owners[0])\n",
    "    # top_owners_amount = grouped_owners.nlargest(10).sum()\n",
    "    # total_burned_amount = grouped_owners.sum()\n",
    "    # distribution_top_10 = top_owners_amount / total_burned_amount if total_burned_amount != 0 else 0\n",
    "\n",
    "    # total_burned_amount = pre_rugpull_data['amount'].sum()\n",
    "    # distribution_top_10 = top_owners_amount / total_burned_amount if total_burned_amount != 0 else 0\n",
    "    \n",
    "    pre_rugpull_data = pre_rugpull_data.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Identify if token0 or token1 is WETH and rename columns\n",
    "    if token0_symbol == 'WETH':\n",
    "        weth_token = 'amount0'\n",
    "        otherToken = 'amount1'\n",
    "    elif token1_symbol == 'WETH':\n",
    "        weth_token = 'amount1'\n",
    "        otherToken = 'amount0'\n",
    "    else:\n",
    "        print(f\"Skipping pool {pool_id} because neither token is WETH\")\n",
    "        continue\n",
    "\n",
    "    # Rename columns\n",
    "    pre_rugpull_data.columns = [col.lower().replace(weth_token, 'wethToken').replace(otherToken, 'otherToken') for col in pre_rugpull_data.columns]\n",
    "\n",
    "    # # Debugging: Print new column names\n",
    "    # print(\"New Column Names:\", pre_rugpull_data.columns)\n",
    "    \n",
    "    # Calculate mean and variance for relevant columns\n",
    "    means = pre_rugpull_data.mean().fillna(0)\n",
    "    variances = pre_rugpull_data.var().fillna(0)  # Replacing NaN with 0\n",
    "\n",
    "    feature_row = {}\n",
    "\n",
    "    feature_row = {'pool_id': pool_id}\n",
    "    for mean in means.index:\n",
    "        feature_row[f'mean_{mean}'] = means[mean]\n",
    "    for variance in variances.index:\n",
    "        feature_row[f'variance_{variance}'] = variances[variance]\n",
    "        \n",
    "    feature_rows.append(feature_row)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "pool_features = pd.DataFrame(feature_rows)\n",
    "\n",
    "print(pool_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_data_with_labels = pd.read_csv('merged_pool_data.csv', low_memory=False)\n",
    "\n",
    "# Merge the DataFrames on the 'pool_id' column\n",
    "merged_data = pool_data_with_labels.merge(pool_features, on='pool_id', how='left')\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file, if needed\n",
    "merged_data.to_csv('merged_pool_data2.csv', index=False)\n",
    "\n",
    "print(\"Merged data saved to merged_pool_data2.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
