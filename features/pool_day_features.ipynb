{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          close       date                      feeGrowthGlobal0X128  \\\n",
      "0  1.078152e-08 2023-06-27  1950663683887649585341457959283660012971   \n",
      "1  9.998356e-05 2021-07-17       11229880233198962195483552111498235   \n",
      "2  8.339408e-01 2022-03-14                                         0   \n",
      "3  1.199533e+00 2022-03-15                                         0   \n",
      "4  1.206535e+00 2022-03-16                                         0   \n",
      "\n",
      "               feeGrowthGlobal1X128    feesUSD          high  \\\n",
      "0  61306156093378588702209917153492  32.158471  9.275127e+07   \n",
      "1                                 0   0.001849  1.000164e+04   \n",
      "2                                 0   0.000000  8.339408e-01   \n",
      "3      6017672668216301140469596925   0.000000  8.336578e-01   \n",
      "4    109367563722438852797476668213   0.000000  8.299676e-01   \n",
      "\n",
      "                                                 id              liquidity  \\\n",
      "0  0x0000d36ab86d213c14d93cd5ae78615a20596505-19535                      0   \n",
      "1  0x0001fcbba8eb491c3ccfeddc5a5caba1a98c4c28-18825  303015134493562686441   \n",
      "2  0x0002e63328169d7feea121f1e32e4f620abf0352-19065     169641513762396911   \n",
      "3  0x0002e63328169d7feea121f1e32e4f620abf0352-19066                      0   \n",
      "4  0x0002e63328169d7feea121f1e32e4f620abf0352-19067   82712555165430868147   \n",
      "\n",
      "            low          open                    sqrtPrice    tick  \\\n",
      "0  9.275127e+07  9.275127e+07    8226586347479349720475099 -183464   \n",
      "1  1.000164e+04  1.000164e+04  792216481398733702759960397  -92110   \n",
      "2  8.339408e-01  8.339408e-01    2743544328858538351424287 -205427   \n",
      "3  8.336578e-01  8.336578e-01    2744009960626817412474522 -205424   \n",
      "4  8.288198e-01  8.299676e-01    2752006904804843592574829 -205366   \n",
      "\n",
      "    token0Price   token1Price  totalValueLockedUSD  txCount  volumeToken0  \\\n",
      "0  9.275127e+07  1.078152e-08            49.340033       22  1.767643e+08   \n",
      "1  1.000164e+04  9.998356e-05            51.020236        2  1.000000e+00   \n",
      "2  8.339408e-01  1.199126e+00             0.000000        1  0.000000e+00   \n",
      "3  8.336578e-01  1.199533e+00             0.000000        3  8.312979e-01   \n",
      "4  8.288198e-01  1.206535e+00             0.000000        6  1.775391e+03   \n",
      "\n",
      "   volumeToken1    volumeUSD                                     pool.id  \n",
      "0      1.699299  3215.847063  0x0000d36ab86d213c14d93cd5ae78615a20596505  \n",
      "1      0.000099     0.184919  0x0001fcbba8eb491c3ccfeddc5a5caba1a98c4c28  \n",
      "2      0.000000     0.000000  0x0002e63328169d7feea121f1e32e4f620abf0352  \n",
      "3      1.000000     0.000000  0x0002e63328169d7feea121f1e32e4f620abf0352  \n",
      "4   2146.820493     0.000000  0x0002e63328169d7feea121f1e32e4f620abf0352  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pool_day_data_df = pd.read_csv('../data/pool_day_data.csv', low_memory=False)\n",
    "rugpull_labels_df = pd.read_csv('../data/rugpulls_with_token_info.csv', low_memory=False)\n",
    "\n",
    "# Convert 'date' to a datetime object\n",
    "pool_day_data_df['date'] = pd.to_datetime(pool_day_data_df['date'], unit='s')\n",
    "\n",
    "# Ensure that 'id' is the correct identifier for pools\n",
    "print(pool_day_data_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rugpull date to datetime\n",
    "rugpull_labels_df['date'] = pd.to_datetime(rugpull_labels_df['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pool 0: Pool ID 0x000c0d31f6b7cecde4645eef0c4ec6a492659d62\n",
      "Number of pools skipped: 0\n",
      "Processing pool 200: Pool ID 0x0a1665e3f54eeb364bec6954e4497dd802840a01\n",
      "Number of pools skipped: 22\n",
      "Processing pool 400: Pool ID 0x152ac8b0358a215fe1f6dd75c8804e4f6ca046eb\n",
      "Number of pools skipped: 42\n",
      "Processing pool 600: Pool ID 0x1f138debc0721b364a967b09402b45069cba7b35\n",
      "Number of pools skipped: 69\n",
      "Processing pool 800: Pool ID 0x291dae7ebf6193910bf69cb14f121c07cfea4de0\n",
      "Number of pools skipped: 89\n",
      "Processing pool 1000: Pool ID 0x3429e990337542434f6bfac5a4d9f14ed285ac7a\n",
      "Number of pools skipped: 115\n",
      "Processing pool 1200: Pool ID 0x3e441eb04d28e1f8cac30f6e32d736a96d8a13b1\n",
      "Number of pools skipped: 134\n",
      "Processing pool 1400: Pool ID 0x46ca72148708fbc450e046fffd264a7821f70ba3\n",
      "Number of pools skipped: 162\n",
      "Processing pool 1600: Pool ID 0x51195c58837babb4ae172c3ac4e29bdc50db4058\n",
      "Number of pools skipped: 189\n",
      "Processing pool 1800: Pool ID 0x5aac14ca709846c709f746350ca1f739462027a3\n",
      "Number of pools skipped: 215\n",
      "Processing pool 2000: Pool ID 0x632881d9f6231bee100cf7d060cc27d86b2a5cdb\n",
      "Number of pools skipped: 236\n",
      "Processing pool 2200: Pool ID 0x6cff6cce76995af3a962e64b956be8b4020889eb\n",
      "Number of pools skipped: 251\n",
      "Processing pool 2400: Pool ID 0x77532d31f043aa870f745615854bbc987aabc14c\n",
      "Number of pools skipped: 272\n",
      "Processing pool 2600: Pool ID 0x811c9e30dad67f0e02b89cf66d42f75c41fc68aa\n",
      "Number of pools skipped: 294\n",
      "Processing pool 2800: Pool ID 0x89cc4216e0a61152fb631c9b3bdc825d5373e43f\n",
      "Number of pools skipped: 315\n",
      "Processing pool 3000: Pool ID 0x943c66d78875ddeb3aede2ac081625d0a947bbad\n",
      "Number of pools skipped: 336\n",
      "Processing pool 3200: Pool ID 0x9e5bfb074aa3a90daf84484f9e099ad39d64d578\n",
      "Number of pools skipped: 355\n",
      "Processing pool 3400: Pool ID 0xa81fb934e257df74dfd113e1dafbc98a5fe10469\n",
      "Number of pools skipped: 380\n",
      "Processing pool 3600: Pool ID 0xb11d15da84a206670beba4e8172c69e653516e80\n",
      "Number of pools skipped: 403\n",
      "Processing pool 3800: Pool ID 0xbadec352e0dd27f6977e14c8cd770f67d9749aae\n",
      "Number of pools skipped: 428\n",
      "Processing pool 4000: Pool ID 0xc4090b37eeff584fc48c58e9d2303acca82247dc\n",
      "Number of pools skipped: 443\n",
      "Processing pool 4200: Pool ID 0xce3dd41cfe1d14e66df18fa66ff5bb1dfd98b880\n",
      "Number of pools skipped: 463\n",
      "Processing pool 4400: Pool ID 0xd86c5cfb2682e4c097177b41bd8127fb56af455b\n",
      "Number of pools skipped: 482\n",
      "Processing pool 4600: Pool ID 0xe2373abf8c81ac76fa7846cda0de14d1255a3ac7\n",
      "Number of pools skipped: 504\n",
      "Processing pool 4800: Pool ID 0xea687867b52edd80365e4d266a60b6a169225051\n",
      "Number of pools skipped: 528\n",
      "Processing pool 5000: Pool ID 0xf3b2204b3b326131829b34f94eda81d65c878e84\n",
      "Number of pools skipped: 550\n",
      "Processing pool 5200: Pool ID 0xfc85f40cd8ae99100100d535b3675ce67c49f33d\n",
      "Number of pools skipped: 576\n",
      "                                      pool_id  \\\n",
      "0  0x000c0d31f6b7cecde4645eef0c4ec6a492659d62   \n",
      "1  0x000ea4a83acefdd62b1b43e9ccc281f442651520   \n",
      "2  0x0025ade782cc2b2415d1e841a8d52ff5dce33dfe   \n",
      "3  0x0068bb604413dfee5c453907bb150d0312a0f257   \n",
      "4  0x0073ce82d9a8ffa9b695cca63cd3993c3eaef4dc   \n",
      "\n",
      "   avg_daily_volume_change_per_swap  \n",
      "0                       -238.254685  \n",
      "1                       -125.100576  \n",
      "2                      -1507.205646  \n",
      "3                       -201.598492  \n",
      "4                      -1358.865825  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "count = 0\n",
    "# Initialize a list to store the feature rows\n",
    "feature_rows = []\n",
    "# Initialize a DataFrame to store the features\n",
    "pool_features = pd.DataFrame()\n",
    "\n",
    "for index, pool_id in enumerate(rugpull_labels_df['pool_id']):\n",
    "    if index % 200 == 0:\n",
    "        print(f\"Processing pool {index}: Pool ID {pool_id}\")\n",
    "        print(f\"Number of pools skipped: {count}\")\n",
    "        # if index > 0: break\n",
    "\n",
    "    rugpull_date = rugpull_labels_df[rugpull_labels_df['pool_id'] == pool_id]['date'].iloc[0] # Get the rugpull date for this pool\n",
    "\n",
    "    token0_symbol = rugpull_labels_df[rugpull_labels_df['pool_id'] == pool_id]['token0.symbol'].iloc[0]\n",
    "    token1_symbol = rugpull_labels_df[rugpull_labels_df['pool_id'] == pool_id]['token1.symbol'].iloc[0]\n",
    "\n",
    "    # print(rugpull_date)\n",
    "    pool_data = pool_day_data_df[pool_day_data_df['pool.id'] == pool_id]\n",
    "\n",
    "    if (len(pool_data) < 3):\n",
    "        # print(f\"Skipping pool {pool_id} because there is no data\")\n",
    "        count += 1\n",
    "        continue\n",
    "\n",
    "    # If rugpull_date is NaT, use all available data; else filter data before rugpull\n",
    "    pre_rugpull_data = pool_data if pd.isna(rugpull_date) else pool_data[pool_data['date'] < rugpull_date]\n",
    "\n",
    "    pre_rugpull_data = pre_rugpull_data.select_dtypes(include=[np.number])\n",
    "    \n",
    "    if pre_rugpull_data.empty:\n",
    "        print(f\"Skipping pool {pool_id} because there is no data before the rugpull date\")\n",
    "        continue\n",
    "    \n",
    "    # Identify if token0 or token1 is WETH and rename columns\n",
    "    if token0_symbol == 'WETH':\n",
    "        weth_token = 'token0'\n",
    "        otherToken = 'token1'\n",
    "    elif token1_symbol == 'WETH':\n",
    "        weth_token = 'token1'\n",
    "        otherToken = 'token0'\n",
    "    else:\n",
    "        print(f\"Skipping pool {pool_id} because neither token is WETH\")\n",
    "        continue\n",
    "\n",
    "    # Rename columns\n",
    "    pre_rugpull_data.columns = [col.lower().replace(weth_token, 'wethToken').replace(otherToken, 'otherToken') for col in pre_rugpull_data.columns]\n",
    "\n",
    "    # # Debugging: Print new column names\n",
    "    # print(\"New Column Names:\", pre_rugpull_data.columns)\n",
    "    \n",
    "    # Calculate mean and variance for relevant columns\n",
    "    # means = pre_rugpull_data.mean()\n",
    "    # variances = pre_rugpull_data.var().fillna(0)  # Replacing NaN with 0\n",
    "\n",
    "    feature_row = {}\n",
    "\n",
    "\n",
    "    pre_rugpull_data['volume_change'] = pre_rugpull_data['volumeusd'].diff()\n",
    "    pre_rugpull_data = pre_rugpull_data.iloc[1:]\n",
    "    pre_rugpull_data['volume_change_per_swap'] = pre_rugpull_data.apply(lambda row: row['volume_change'] / row['txcount'] if row['txcount'] > 0 else np.nan, axis=1)\n",
    "    avg_daily_volume_change_per_swap = pre_rugpull_data['volume_change_per_swap'].mean()\n",
    "\n",
    "    # Create a feature row for this pool\n",
    "    feature_row = {'pool_id': pool_id, 'avg_daily_volume_change_per_swap': avg_daily_volume_change_per_swap if not np.isnan(avg_daily_volume_change_per_swap) else 0}\n",
    "    # for mean in means.index:\n",
    "    #     feature_row['mean_' + mean] = means[mean]\n",
    "    # for variance in variances.index:\n",
    "    #     feature_row['variance_' + variance] = variances[variance]\n",
    "\n",
    "    feature_rows.append(feature_row)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "pool_features = pd.DataFrame(feature_rows)\n",
    "\n",
    "print(pool_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool features saved to pool_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "pool_features.to_csv('pool_features.csv', index=False)\n",
    "\n",
    "print(\"Pool features saved to pool_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            avg_daily_volume_change_per_swap\n",
      "pool_id                                                                     \n",
      "0x000c0d31f6b7cecde4645eef0c4ec6a492659d62                       -238.254685\n",
      "0x000ea4a83acefdd62b1b43e9ccc281f442651520                       -125.100576\n",
      "0x0025ade782cc2b2415d1e841a8d52ff5dce33dfe                      -1507.205646\n",
      "0x0068bb604413dfee5c453907bb150d0312a0f257                       -201.598492\n",
      "0x0073ce82d9a8ffa9b695cca63cd3993c3eaef4dc                      -1358.865825\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'rugpull_labels_df' has columns 'pool_id' and 'rugpull' and that 'pool_id' is unique\n",
    "# Also assuming 'pool_features' has a 'pool_id' column\n",
    "\n",
    "# First, set the index to 'pool_id' for merging\n",
    "rugpull_labels_df.set_index('pool_id', inplace=True)\n",
    "pool_features.set_index('pool_id', inplace=True)\n",
    "\n",
    "# # Merge the DataFrames on 'pool_id'\n",
    "# pool_features_with_labels = pool_features.join(rugpull_labels_df['rugpull'])\n",
    "\n",
    "# # Reset index if you want 'pool_id' back as a column\n",
    "# pool_features_with_labels.reset_index(inplace=True)\n",
    "\n",
    "print(pool_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rugpull\n",
      "False    3955\n",
      "True      730\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # print amount true\n",
    "# print(pool_features_with_labels['rugpull'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame with the rugpull labels to a CSV\n",
    "# pool_features_with_labels.to_csv('pool_features_with_labels.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      pool_id  \\\n",
      "0  0x000c0d31f6b7cecde4645eef0c4ec6a492659d62   \n",
      "1  0x000ea4a83acefdd62b1b43e9ccc281f442651520   \n",
      "2  0x0025ade782cc2b2415d1e841a8d52ff5dce33dfe   \n",
      "3  0x0068bb604413dfee5c453907bb150d0312a0f257   \n",
      "4  0x0073ce82d9a8ffa9b695cca63cd3993c3eaef4dc   \n",
      "\n",
      "   avg_daily_volume_change_per_swap    mean_close  mean_feesusd     mean_high  \\\n",
      "0                       -238.254685  3.935271e+04    526.090312  2.268379e+37   \n",
      "1                       -125.100576  3.889389e+01     39.890917  9.373465e+35   \n",
      "2                      -1507.205646  4.930590e+06     59.646370  3.105088e-07   \n",
      "3                       -201.598492  4.765163e+03      0.713109  7.901676e+04   \n",
      "4                      -1358.865825  4.087737e+06    266.206755  3.800094e+07   \n",
      "\n",
      "       mean_low     mean_open      mean_tick  mean_wethTokenprice  \\\n",
      "0  2.082785e-05  1.134189e+37  185303.566667         2.202031e-05   \n",
      "1  2.336523e+03  9.373465e+35  -74334.972452         4.906514e-04   \n",
      "2  2.786997e-07  3.015521e-07  152633.607843         2.876540e-07   \n",
      "3  7.702307e+04  7.817209e+04   25752.684211         1.342957e-05   \n",
      "4  3.314526e+07  3.434370e+07 -173720.000000         3.097993e-08   \n",
      "\n",
      "   mean_otherTokenprice  ...  mean_amountusd_y  mean_logindex_y  \\\n",
      "0          4.498062e+04  ...       2553.530358       235.383333   \n",
      "1          2.395095e+03  ...     125686.442198       263.142857   \n",
      "2          5.979833e+06  ...       5359.800201       285.555556   \n",
      "3          7.776255e+04  ...          0.000000       329.000000   \n",
      "4          3.661731e+07  ...       2290.055857       414.000000   \n",
      "\n",
      "   mean_ticklower_y  mean_tickupper_y  variance_wethToken_y  \\\n",
      "0      70506.666667          120800.0             10.596523   \n",
      "1     -82771.428571          -72920.0          21014.706919   \n",
      "2     -10980.000000          179880.0             37.677600   \n",
      "3     130120.000000          145095.0              0.000000   \n",
      "4    -648666.666667          567200.0              3.752031   \n",
      "\n",
      "   variance_otherToken_y  variance_amountusd_y  variance_logindex_y  \\\n",
      "0           9.533187e+09          3.015475e+07         11146.274294   \n",
      "1           7.367546e+07          3.088336e+11         12541.428571   \n",
      "2           7.078090e+14          2.960316e+08         24983.256410   \n",
      "3           2.002488e+10          0.000000e+00         35912.000000   \n",
      "4           7.196837e+15          1.325984e+07         19762.000000   \n",
      "\n",
      "   variance_ticklower_y  variance_tickupper_y  \n",
      "0          1.761644e+10          1.013570e+10  \n",
      "1          4.835938e+07          4.605960e+07  \n",
      "2          0.000000e+00          1.550415e+08  \n",
      "3          2.293654e+10          2.651214e+10  \n",
      "4          1.365556e+11          2.457600e+11  \n",
      "\n",
      "[5 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "# print(len(pool_features_with_labels))\n",
    "# Load merged_dat2.csv into a DataFrame\n",
    "merged_data2_df = pd.read_csv('../data/merged_pool_data2.csv', index_col='pool_id')\n",
    "\n",
    "pool_features_with_merged_data = pool_features.join(merged_data2_df, how='inner')\n",
    "\n",
    "# Reset index if you want 'pool_id' back as a column\n",
    "pool_features_with_merged_data.reset_index(inplace=True)\n",
    "\n",
    "# Check the first few rows of the joined DataFrame\n",
    "print(pool_features_with_merged_data.head())\n",
    "\n",
    "# Save the DataFrame with the rugpull labels to a CSV\n",
    "pool_features_with_merged_data.to_csv('../data/merged_pool_data3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
